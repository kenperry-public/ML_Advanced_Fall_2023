{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mesh TensorFlow\n",
    "\n",
    "Running models of sufficiently large size pose practical difficulties\n",
    "- taking advantage of the inherent parallelism afforded by multiple computational units (processors, cores)\n",
    "- insufficient memory per processor to contain a model's parameters\n",
    "\n",
    "Fortunately, there are programming abstractions that mitigate these difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Mesh-TensorFlow](https://arxiv.org/pdf/1811.02084.pdf) is an extension of TensorFlow\n",
    "that provides a clean API for dealing with various forms of parallelism.\n",
    "\n",
    "We provide a very brief introduction\n",
    "- motivated by its use in implementing the MoE in the FFN of the Switch Transformer\n",
    "\n",
    "Note that this is an extension of\n",
    "\"low-level\" TensorFlow, **not** higher level Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forms of parallelism\n",
    "\n",
    "We assume \n",
    "- a collection of computational units (referred to as *processors*)\n",
    "- that are able to communicate with one another\n",
    "    - by a arbitary communication fabric\n",
    "    \n",
    "We want our models to be able to take advantage of the multiple processors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The two common forms of parallelism are\n",
    "- data parallelism\n",
    "    - each example in a mini-batch is independent\n",
    "    - split the batch across processors\n",
    "- model parallelism\n",
    "    - when a model's parameters are too large to fit into memory of a single processor\n",
    "    - split the parameters (and computation) across multiple processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate: Consider\n",
    "- A batch $\\X$ of $m$ examples, each a one-dimensional vector of length $n$: $\\X \\in \\Reals^{m \\times n}$\n",
    "- A vector $\\w$ of length $n$: $\\w \\in \\Reals^n$\n",
    "    - e.g., one row of a weight matrix $\\W$ implementing the `Dense` operation of $\\X * \\W$\n",
    "- We want to compute the dot product of every example with $\\w$\n",
    "$$\n",
    "\\X^\\ip \\cdot \\w\n",
    "$$\n",
    "for every example $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data parallelism\n",
    "\n",
    "This is the simplest form of parallelism\n",
    "- split the examples into groups\n",
    "$$\n",
    "\\X = \\begin{pmatrix} \\\\\n",
    "    \\X^{(0:g-1)} \\\\\n",
    "    \\X^{(g:2*g-1)} \\\\\n",
    "    \\vdots\n",
    "    \\\\X^{(m-g:m-1)}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dispatch\n",
    "- a single group (e.g., $\\X^{(s:e)}$) to a single processor\n",
    "- the weights $\\w$ to each processor\n",
    "\n",
    "Each processor (e.g., the one assigned examples  $\\X^{(s:e)}$) computes\n",
    "$$\n",
    "\\X^{(s:e)} * \\w\n",
    "$$\n",
    "\n",
    "\n",
    "n.b., multiplication of matrix and vector results in vector output\n",
    "- that is dot product of each row of matrix and the right vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model parallelism\n",
    "\n",
    "Suppose dimension $n$ is so large that a single processor's memory cannot accommodate either vector\n",
    "- corresponding to a single example $\\X^\\ip$\n",
    "- corresponding to $\\w$\n",
    "\n",
    "We will split each vector into (horizontal) groups\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\X^\\ip & = & [ \\X^\\ip_{0:g-1}, \\X^\\ip_{g,2*g-1}, \\ldots, \\X^\\ip_{n-g, n-1} ] \\\\\n",
    "\\w     & = & [ \\w_{0:g-1},     \\w_{g,2*g-1},     \\ldots, \\w_{n-g, n-1} ] \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dispatch a group\n",
    "- $\\X^\\ip_{s:e}$ and $\\w_{s:e}$ to a single processor $p$\n",
    "which computes the dot product\n",
    "$$\n",
    "h^{(p)} = \\X^\\ip_{s:e} \\cdot \\w_{s:e}\n",
    "$$\n",
    "\n",
    "Note that $h^{(p)}$ is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Gather* (using the communication network) the $h_{(p)}$ from all processors $p$\n",
    "$$\n",
    "\\h = [ h^{(0)}, h^{(1)}, \\ldots, h^{(\\frac{n}{g})} ]\n",
    "$$\n",
    "\n",
    "*Reduce* the vector $\\h$ to a scalar\n",
    "- by summing over its elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward pass\n",
    "\n",
    "We have illustrated the fundamental ideas of parallelism \n",
    "- using a computation from the forward pass of a Neural Network\n",
    "\n",
    "The backward pass (gradient flow) can also be implemented\n",
    "- but requires Gather and Reduce operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining parallelism in Mesh-TensorFlow\n",
    "\n",
    "As per our illustration, implementing the two forms of parallelism involves\n",
    "- splitting Tensors\n",
    "- and possibly gathering/reducing sub-results\n",
    "\n",
    "Mesh-TensorFlow provides a simple notation for describing how to split a Tensor\n",
    "- everything else (dispatching, gathering, reducing) is automatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A Tensor has multiple dimensions.\n",
    "\n",
    "In Mesh-TensorFlow, we can give each dimension a *name* and a size.\n",
    "\n",
    "For example, the first dimension of most Tensors in a Neural Network is the \"batch\" dimension.\n",
    "- We can specify a dimension named \"batch\", of size 100:\n",
    "\n",
    "        batch_dim = mtf.Dimension(\"batch\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mesh-TensorFlow also allows the user to specify\n",
    "- The logical (not physical connectivity) organization of processors.\n",
    "    - Treating 4 processors as a one-dimensional vector\n",
    "\n",
    "             mesh_shape = [(\"all_processors\", 4)]\n",
    "- *Layout* rules: how a named dimension is split into groups\n",
    "    - To specify data parallelism (split batch across processors)\n",
    "    \n",
    "            layout_rules = [(\"batch\", \"all_processors\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mesh-TensorFlow thus provides a simple but powerful API\n",
    "- for distributing data and computation\n",
    "- across multiple processors\n",
    "\n",
    "We illustrate with an example \n",
    "taken from the [Mesh TensorFlow github](https://github.com/tensorflow/mesh#example-network-mnist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Mesh-TensorFlow program for MNIST classification task](https://github.com/tensorflow/mesh#example-network-mnist)\n",
    "\n",
    "\n",
    "### Name the dimensions\n",
    "\n",
    "        # tf_images is a tf.Tensor with shape [100, 28, 28] and dtype tf.float32\n",
    "        # tf_labels is a tf.Tensor with shape [100] and dtype tf.int32\n",
    "        graph = mtf.Graph()\n",
    "        \n",
    "        mesh = mtf.Mesh(graph, \"my_mesh\")\n",
    "        \n",
    "        batch_dim = mtf.Dimension(\"batch\", 100)\n",
    "        \n",
    "        rows_dim = mtf.Dimension(\"rows\", 28)\n",
    "        cols_dim = mtf.Dimension(\"cols\", 28)\n",
    "        \n",
    "        hidden_dim = mtf.Dimension(\"hidden\", 1024)\n",
    "        \n",
    "        classes_dim = mtf.Dimension(\"classes\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Compute logits  loss, and update weights via Gradient Descent      \n",
    "        images = mtf.import_tf_tensor(\n",
    "            mesh, tf_images, shape=[batch_dim, rows_dim, cols_dim])\n",
    "        labels = mtf.import_tf_tensor(mesh, tf_labels, [batch_dim])\n",
    "        \n",
    "        w1 = mtf.get_variable(mesh, \"w1\", [rows_dim, cols_dim, hidden_dim])\n",
    "        w2 = mtf.get_variable(mesh, \"w2\", [hidden_dim, classes_dim])\n",
    "        \n",
    "        # einsum is a generalization of matrix multiplication (see numpy.einsum)\n",
    "        hidden = mtf.relu(mtf.einsum(images, w1, output_shape=[batch_dim, hidden_dim]))\n",
    "        logits = mtf.einsum(hidden, w2, output_shape=[batch_dim, classes_dim])\n",
    "        \n",
    "        loss = mtf.reduce_mean(mtf.layers.softmax_cross_entropy_with_logits(\n",
    "            logits, mtf.one_hot(labels, classes_dim), classes_dim))\n",
    "            \n",
    "        w1_grad, w2_grad = mtf.gradients([loss], [w1, w2])\n",
    "        update_w1_op = mtf.assign(w1, w1 - w1_grad * 0.001)\n",
    "        update_w2_op = mtf.assign(w2, w2 - w2_grad * 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Specify mesh of processors and layout of computation\n",
    "\n",
    "#### Layout for data parallelism\n",
    "\n",
    "The following layout implements *data parallelism*\n",
    "- Any Tensor with a dimension named \"batch\" dimension\n",
    "    - `images, h, logits` and their gradients\n",
    "- is split across all devices (\"all_processors\")\n",
    "\n",
    "\n",
    "        devices = [\"gpu:0\", \"gpu:1\", \"gpu:2\", \"gpu:3\"]\n",
    "        mesh_shape = [(\"all_processors\", 4)]\n",
    "        \n",
    "        layout_rules = [(\"batch\", \"all_processors\")]\n",
    "        \n",
    "        mesh_impl = mtf.placement_mesh_impl.PlacementMeshImpl(\n",
    "            mesh_shape, layout_rules, devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Layout for model parallelism\n",
    "\n",
    "Alternatively, we can use a layout to implement *model parallelism*\n",
    "- Any Tensor with a dimension named \"hidden\" is split\n",
    "    - `hidden w1, w2`\n",
    "- is split across all devices (\"all_processors\")\n",
    "\n",
    "        layout_rules=[(\"hidden\", \"all_processors\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Layout for data and model parallelism\n",
    "\n",
    "We create a 2D mesh of processors\n",
    "- rows are named `processor_rows`, columns are named `processor_cols`\n",
    "        mesh_shape = [(\"processor_rows\", 2), (\"processor_cols\", 2)]\n",
    "\n",
    "And use the layout\n",
    "\n",
    "        layout_rules = [(\"batch\", \"processor_rows\"), (\"hidden\", \"processor_cols\")]\n",
    "        \n",
    "Layout splits\n",
    "- Tensors with a \"batch\" dimension across the processor rows (data parallelism)\n",
    "    - replicating them for every processor column\n",
    "- Tensors with a \"hidden\" dimension across processor columns (model parallelism)\n",
    "    - replicating them for every processor row\n",
    "        layout_rules = [(\"batch\", \"processor_rows\"), (\"hidden\", \"processor_cols\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "So the processors\n",
    "- in first row has half the examples\n",
    "- in the second row has half the examples\n",
    "- in the first column has half the weights\n",
    "- in the second column has half the weights\n",
    "\n",
    "So each quadrant (one processor)\n",
    "- computes the dot product of half the examples on half the weights\n",
    "- for the `hidden` tensor\n",
    "    - has **both** \"batch\" and \"hidden\" dimensions\n",
    "    - so is distributed across all 4 quadrants of the mesh\n",
    "\n",
    "\n",
    "These are combined into a single result batch via a reduction\n",
    "- `allreduce` operation\n",
    "- communication across processors\n",
    "    - or partially reduced dot products (i.e., scalars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### \"Lower\" the logical layout unto the physical  devices\n",
    "\n",
    "        lowering = mtf.Lowering(graph, {mesh:mesh_impl})\n",
    "        \n",
    "        tf_update_ops = [lowering.lowered_operation(update_w1_op),\n",
    "                         lowering.lowered_operation(update_w2_op)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Illustrations of layouts\n",
    "\n",
    "[Appendix A](https://arxiv.org/pdf/1811.02084.pdf#page=11) provides nice illustrations\n",
    "- of layouts\n",
    "- for an example similar to the MNIST program\n",
    "\n",
    "**Notation**\n",
    "- Tensors are split by vertical and horizontal lines\n",
    "- Blue integers indicate which processor a piece of a Tensor has been dispatched to\n",
    "    - some pieces are dispatched to multiple processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data parallelism\n",
    "\n",
    "<table>\n",
    "    <center><strong>Data parallelism</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/mtf_data_parallel.png\" width=80%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://arxiv.org/pdf/1811.02084.pdf#page=12\n",
    "</table>\n",
    "\n",
    "- $\\X$ (and result $\\h = \\X * \\W$) are split along the batch dimension\n",
    "    - groups assigned to processors $0, 1$\n",
    "- $\\W$ is *not split*\n",
    "    - full $\\W$ is replicated across processors $0, 1$\n",
    "    \n",
    "Thus, processors $0, 1$ are able to compute their subset of $\\X$, times $\\W$\n",
    "- resulting $\\h$ is split (along batch dimension) across  processors $0, 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model parallelism\n",
    "\n",
    "<table>\n",
    "    <center><strong>Model parallelism</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/mtf_model_parallel.png\" width=80%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://arxiv.org/pdf/1811.02084.pdf#page=12\n",
    "</table>\n",
    "\n",
    "- $\\X$ is not split\n",
    "    - replicated to processors $0, 1$\n",
    "- $\\W$ is split (across columns)\n",
    "    - groups assigned to processors $0, 1$\n",
    "    \n",
    "Thus\n",
    "- processor $0$ can compute the full $\\X$ times the first group of $\\W$\n",
    "- processor $0$ can compute the full $\\X$ times the second group of $\\W$\n",
    "- resulting in $\\h$ split across processors $0, 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data and Model parallelism\n",
    "\n",
    "Here, 4 processors are arranged into a $2 \\times 2$ mesh.\n",
    "\n",
    "<table>\n",
    "    <center><strong>Data and Model parallelism</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/mtf_data_and_model_parallel.png\" width=80%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://arxiv.org/pdf/1811.02084.pdf#page=12\n",
    "</table>\n",
    "\n",
    " $\\X$ (and result $\\h = \\X * \\W$) are split along the batch dimension\n",
    "- one group is replicated\n",
    "    - assigned to processors $0, 1$ \n",
    "- second group is replicated\n",
    "    - assigned to processors $2, 3$\n",
    "- $\\W$ is split (across columns)\n",
    "- one group is replicated\n",
    "    - assigned to processors $0, 2$\n",
    "- second group is replicated\n",
    "    - assigned to processors (1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus\n",
    "- processor $0$ can compute first group of $\\X$ times first group of $\\W$\n",
    "- processor $1$ can compute first group of $\\X$ times second group of $\\W$\n",
    "- processor $2$ can compute second group of $\\X$ times first group of $\\W$\n",
    "- processor $3$ can compute second group of $\\X$ times second group of $\\W$\n",
    "\n",
    "Note that result $\\h$ is split across **both*\n",
    "- batch dimension (due to the data parallel split of $\\X$)\n",
    "- model dimension (due to model parallel split of $\\W$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
