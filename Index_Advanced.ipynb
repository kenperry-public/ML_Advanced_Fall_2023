{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Classical Machine Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We give a brief introduction to the course.\n",
    "\n",
    "We then present the key concepts that form the basis for this course\n",
    "- For some: this will be review\n",
    "- For others: it will be a preview\n",
    "\n",
    "## Intro to Advanced Course\n",
    "\n",
    "- [Introduction to Advanced Course](Intro_Advanced.ipynb)\n",
    "- [Review and Preview](Review_Advanced.ipynb)\n",
    "\n",
    "\n",
    "You may want to run your code on Google Colab in order to take advantage of powerful GPU's.\n",
    "\n",
    "Here are some useful tips:\n",
    "\n",
    "[Google Colab tricks](Colab_practical.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Review/Preview of concepts from Intro Course\n",
    "\n",
    "### [Transfer Learning: Review ](Review_TransferLearning.ipynb)\n",
    "  \n",
    "### [Transformers: Review](Review_Transformer.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "- Attention\n",
    "    - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)  \n",
    "- Transfer Learning    \n",
    "    - [Sebastian Ruder: Transfer Learning](https://ruder.io/transfer-learning/)\n",
    "    \n",
    "**Further reading**\n",
    "- Attention\n",
    "    - [Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "    - Geron Chapter 16\n",
    "    - [An Analysis of BERT's Attention](https://arxiv.org/pdf/1906.04341.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2: Review (continued)\n",
    "\n",
    "**Preview**\n",
    "\n",
    "There is lots of interest in Large Language Models (e.g., ChatGPT).  These are based on an architecture called the Transformer.  We will introduce the Transformer and demonstrate some amazing results achieved by using Transformers to create Large Language Models.\n",
    "\n",
    "Attention is a mechanism that is a core part of the Transformer.  We will begin by first introducing Attention.\n",
    "\n",
    "We will then take a detour and study the Functional model architecture of Keras.  Unlike the Sequential model, which is an ordered sequence of Layers, the organization of blocks in a Functional model is more general.  The Advanced architectures (e.g., the Transformer) are built using the Functional model.\n",
    "\n",
    "Once we understand the technical prerequisites, we will examine the code for the Transformer.\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We continue the review/preview of key concepts that we started last week.\n",
    "- We will *re-start* the module on Attention\n",
    "\n",
    "Our ultimate goal is to introduce the Transformer (which uses Attention heavily) in theory, and demonstrate its use in Large Language Models.\n",
    "\n",
    "## Review continued\n",
    "\n",
    "### [Transformers: Review](Review_Transformer.ipynb)\n",
    "\n",
    "### [Natural Language Processing: Review](Review_NLP.ipynb)\n",
    "\n",
    "### [Language Models, the future (present ?) of NLP: Review](Review_LLM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
