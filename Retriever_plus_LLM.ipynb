{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Retriever-Generator in practice: studying some interesting models\n",
    "\n",
    "We introduced the [Retriever-Generator Architecture](LLM_plus_Extra_Parametric.ipynb#Retriever-Generator-Architecture:-high-level-view) in a previous module.\n",
    "- factual knowledge about the \"world\"\n",
    "- is obtained by an external mechanism\n",
    "- rather than stored in a model's parameters at training time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Retriever-Generator model consists of two sub-models working in concert.\n",
    "\n",
    "We can illustrate this with a model for the Question Answering task.\n",
    "\n",
    "A single NN (non-retrieval) model directly generates output answer $\\y$ given question $\\x$, computing\n",
    "$$\n",
    "\\pr{\\y | \\x }\n",
    "$$\n",
    "\n",
    "By contrast: the Retriever-Generator model has two separate steps, each executed by a sub-model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Retriever* Neural Network\n",
    "- takes the questions $\\x$\n",
    "- returns a set $\\z$ consisting of the top $K$ items in the Knowledge Store that are most relevant to $\\x$\n",
    "$$\n",
    "\\z = \\text{Retriever}(\\x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Generator*: a Neural Network\n",
    "- takes the question $\\x$ and relevant items $\\z$\n",
    "- returns $\\y$: the text that is the answer to questions $\\x$\n",
    "\n",
    "$$\n",
    "\\y = \\text{Generator}( \\x, \\z)\n",
    "$$\n",
    "\n",
    "The model computes\n",
    "$$\n",
    "\\pr{ \\y | \\x, \\text{Retriever}(\\x) }\n",
    "$$\n",
    "the output answer conditional on the question and top facts retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this module\n",
    "- we explore each sub-model in greater detail\n",
    "- explain some interesting new models in terms of the Retriever-Generator architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Retriever: Retrieving World Knowledge\n",
    "\n",
    "An issue that must be addressed by all models\n",
    "- How to retrieve world knowledge relevant to a question.\n",
    "\n",
    "Regardless of how the relevant knowledge is retrieved, the goal is \n",
    "to retrieve the $K$ items most relevant to question $\\x$.\n",
    "- $\\z$ is the subset of the knowledge store containing the $K$ items that most closely match question $\\x$\n",
    "- called the *context* (in which the question is answered)\n",
    "\n",
    "**Notation summary**\n",
    "\n",
    "term | dimension &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | meaning \n",
    ":---|:---|:---\n",
    "$\\x$ |   | input sequence: question\n",
    "$p$  |   | knowledge store: set of \"documents\" (e.g., documents, Web pages)\n",
    "     |   | $p = \\{ p^\\ip | 1 \\le i \\le P \\}$\n",
    "$\\z$ | $K \\times ?$  | set of  \"items\" retrieved from $p$ (typical: top $K$ matches)\n",
    "      |              | $\\z = \\{ \\z^\\ip | 1 \\le i \\le K \\}$\n",
    "      |              | $\\z \\subset p$\n",
    "$\\y$ | $N \\times ? $ | target sequence: answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A particular challenge of non-parametric World Knowledge is  the **length** of documents in the Knoweldge Store\n",
    "-  may exceed the LLM maximum input length\n",
    "- the document is only *part* of the input\n",
    "    - question is also part of the input\n",
    "        \n",
    "To account for this the \"items\" retrieved in $\\z$\n",
    "- may be \"chunks\" of text (long text divided into pieces)\n",
    "- \"passages\": short sequence of text extracted from an item in the Knowledge Store\n",
    "\n",
    "We will use the term *item* to denote shorter blocks of text, extracted from documents in $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Static, non-parametric knowledge\n",
    "\n",
    "This approach uses an external (to the model) *fixed* knowledge store.\n",
    "- stores documents or sentence fragments\n",
    "\n",
    "How do we find the $K$ items in the store most relevant to questions $\\x$ ?\n",
    "\n",
    "We will define the *Retriever* as the mechanism for finding these matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sentence embeddings\n",
    "\n",
    "A *sentence embedding* is a fixed length representation of a block of text\n",
    "\n",
    "This is a generalization of *word embeddings*.\n",
    "\n",
    "Given\n",
    "- a sentence embedding of question $\\x$\n",
    "- a sentence embedding of each item in the knowledge store\n",
    "\n",
    "we can use a distance metric (e.g., Dot product) of the two embeddings to find\n",
    "the items most similar to $\\x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Quick survey on constructing sentence embeddings**\n",
    "\n",
    "Assuming we have an embedding for each word in the block of text\n",
    "- a simple sentence embedding can be the average (across the word embeddings)\n",
    "    - weakness: does not respect ordering of words\n",
    "\n",
    "Rather than using embeddings of words in isolation, \n",
    "a better approach is to use [Contextualized Word Representations](NLP_Word_Representations.ipynb#Contextualized-representations) of each word\n",
    "- the representation of a word in *context*\n",
    "    - of the preceding words\n",
    "    - of the trailing words\n",
    "    \n",
    "An Encoder Transformer produces a representation of each position of the block that takes the entire block\n",
    "into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So passing  a sentence of length $T$ ($\\x$ or an element of set $\\z$) through an Encoder Transformer\n",
    "- results in a vector of length $T$ contextualized representations\n",
    "\n",
    "To create a fixed length representation: we need to eliminate the $T$ dimension\n",
    "- pooling\n",
    "    - average, max\n",
    "- The representation of the `<START>` or `<END>` tokens that bracket the block\n",
    "    - Can be a proxy for the representation of the sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $E$ be an \"Encoder\" that produces a sentence embedding.\n",
    "\n",
    "The distance metric is defined as\n",
    "$$\n",
    "D(\\x, \\z^\\ip) = E( \\x ) \\cdot E( \\x^\\ip )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a \n",
    "[paper](https://arxiv.org/pdf/1908.10084.pdf)\n",
    "with one type of Sentence Embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Dense Passage Retrieval (DPR)](https://arxiv.org/pdf/2004.04906.pdf)\n",
    "\n",
    "The Sentence Embedding is simple but has one potential drawback\n",
    "- questions $\\x$ \n",
    "- are probably different in nature (e.g., length) than the items (e.g., full documents)\n",
    "\n",
    "So the dot product of sentence embeddings of a short question and a long document may not be ideal\n",
    "- one relevant passage in a long document containing many irrelevant passages \n",
    "- may result in a low dot product\n",
    "- making it similar to the dot product with an irrelevant document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One possible solution is to create a model\n",
    "- that extracts the relevant passage from a document of multiple passages\n",
    "- output a *span*\n",
    "    - start and end position (within item) of relevant passage\n",
    "    \n",
    "One could imagine creating a training set for such a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is another alternative:\n",
    "\n",
    "The DPR approach is similar to Sentence Embeddings except\n",
    "- two different Encoders are used\n",
    "- $E_q$ for questions, $E_p$ for items\n",
    "- both are Neural Networks\n",
    "    - e.g., fine-tuned LLM's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The resulting distance metric becomes\n",
    "\n",
    "$$\n",
    "D(\\x, \\z^\\ip) = E_q( \\x ) \\cdot E_p( \\x^\\ip )\n",
    "$$\n",
    "\n",
    "We jointly train $E_q$ and $E_p$ to create embeddings with high dot products when $\\z^\\ip$ is relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximizing the dot product\n",
    "\n",
    "The Retrievers we have define thus far are essentially Multinomial Classifiers over $P$ discrete items\n",
    "- producing a vector of probabilities (e.g., softmax of the dot products of $\\x$ and elements of $p$)\n",
    "\n",
    "If $p^\\ip$ is relevant to questions $\\x$, we want\n",
    "- it's probability to be high\n",
    "- the probability of $p^{(i')}$ to be low, where $i' \\ne i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called a *Contrastive Objective*\n",
    "- creating a contrast (in magnitude of probability) between matches and non-matches\n",
    "\n",
    "A *triplet objective* can be defined\n",
    "- for question $\\x$\n",
    "- matching passage $p^\\ip$\n",
    "- non-matching passage $p^{(i')}$\n",
    "\n",
    "as\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\max{}(0, C) & & \\text{where} \\\\\n",
    "C =   &\\\\\n",
    "          & D \\left( E_q(\\x), E_p( p^\\ip ) \\right) &  \\text{Distance between } \\x \\text{ and match } p^\\ip \\\\\n",
    "          &  - \\\\\n",
    "          & D \\left( E_q(\\x), E_p( p^{(i')} ) \\right), &  \\text{Distance between } \\x \\text{ and non-match } p^{(i')}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "The triplet objective is *minimized* when\n",
    "- $E_q(\\x)$ and $E_p( p^\\ip )$ are close\n",
    "- $E_q(\\x)$ and $E_p( p^{(i')} )$ are far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A training example for a NN using this objective would be\n",
    "$$\n",
    "\\langle\n",
    "\\x,\n",
    "p^+,\n",
    "p^{-,1}\n",
    "\\ldots\n",
    "p^{-, n'}\n",
    "\\rangle\n",
    "$$\n",
    "- a questions $\\x$\n",
    "- a positive (matching) passage $p^+$\n",
    "- $n'$ negative (non-matching)passages\n",
    "$\n",
    "p^{-,1}\n",
    "\\ldots\n",
    "p^{-, n'}\n",
    "$\n",
    "\n",
    "We convert dot products into probabilities via the softmax\n",
    "$$ \n",
    "\\frac\n",
    "{ \\exp \\left( D ( E_q(\\x), E_p( p^+ ) )  \\right) }\n",
    "{ \\exp \\left( D ( E_q(\\x), E_p( p^+ ) )  \\right)\n",
    "+\n",
    "\\sum_{i'=1}^{n'} { \\exp \\left( D ( E_q(\\x), E_p( p^{-,i'} ) ) \\right) }\n",
    "}\n",
    "$$\n",
    "and use Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [In-batch negatives trick](https://arxiv.org/pdf/2004.04906.pdf#page=3)\n",
    "\n",
    "The choice of negative passages $p^{-,1}\n",
    "\\ldots{}\n",
    "p^{-, n'}\n",
    "$\n",
    "- is arbitrary\n",
    "- labor-intensive\n",
    "\n",
    "One can get away with examples that provide **no explicit** negative passages with the following trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When using mini-batch gradient descent, there are B examples per mini-batch\n",
    "\n",
    "Consider example $i$ in the batch\n",
    "- with questions $\\x^\\ip$\n",
    "- Let $p^{\\ip, +}$ denote the positive passage for $\\x^\\ip$\n",
    "- Use $p^{(i'),+}$ \n",
    "    - the positive passage for example $i' \\ne i$\n",
    "    - as a negative examples for $\\x^\\ip$\n",
    "\n",
    "Thus, we the negative passages for an example are implicitly obtained from other examples in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This can also be computationally efficient\n",
    "- Can compute the dot products as one big matrix multiplication\n",
    "\n",
    "**n.b., this In-batch negative trick was also used in** [CLIP](CLIP.ipynb#Pseudo-code-for-Pre-Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dynamic, non-parametric knowledge\n",
    "\n",
    "This approach uses \n",
    "- a constantly updated source of knowledge, e.g., the Web.\n",
    "- to create the set $\\z$ of items in the Knowledge Store most relevant to question $\\x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea is to train the Retriever NN\n",
    "- to create a query to a Search Engine (e.g., Bing)\n",
    "- Scroll through top results\n",
    "- Visit a result\n",
    "    - issue a search for a text string within the result\n",
    "    - select a neighborhood of the result around the search\n",
    "    - adding the neighborhood as a 'reference\" (element of set $\\z$)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will describe this Web Retriever more in depth in the discussion of WebGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generator: Generating the answers\n",
    "\n",
    "The role of the Generator is to create the answer $\\y$, conditioned on\n",
    "- question $\\x$\n",
    "- the set of relevant passages $\\z$\n",
    "\n",
    "$$\n",
    "\\pr{\\y | \\x, \\text{Retriever}(\\x)}\n",
    "$$\n",
    "\n",
    "We have already seen how LLM's can generate answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [RETRO (Deepmind)](https://download.arxiv.org/pdf/2112.04426v3.pdf)\n",
    "\n",
    "RETRO stands for the **R**etrival **E**nhanced **TR**ansf**O**rmer\n",
    "\n",
    "By using a non-parametric Knowledge Store, \n",
    "- RETRO is able to match GPT-3 performance on some benchmarks\n",
    "- using only 4% (i.e., 7.5B vs 175B) of the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model summary: high-level (approximate)\n",
    "\n",
    "Retriever\n",
    "- **static** non-parametric knowledge\n",
    "    - 2 trillion tokens.\n",
    "- uses similarity of **sentence embeddings** of query and items in Knowledge Store to find relevant items\n",
    "\n",
    "Generator\n",
    "- Encoder/Decoder Transformer\n",
    "    - Encoder\n",
    "        - $\\z$ (items returned by Retriever as relevant to $\\x$) passed through Encoder\n",
    "        - Latent states of Encoder become Keys and Values for Attention\n",
    "    - Decoder\n",
    "        - Attention to input (partially built $\\y$)\n",
    "        - Cross Attention (Decoder-Encoder Attention)\n",
    "            - to items returned by Retriever\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Mode of operation of RETRO vs standard Transformer**\n",
    "\n",
    "In a non-retrieval Transformer (parametric knowledge) with a Language Modeling objective:\n",
    "- usually a [Decoder style Transformer](https://arxiv.org/pdf/1801.10198.pdf#page=4)\n",
    "    - auto-regressively extends partial output one token at a time\n",
    "        - on iteration $\\tt$: generates $\\y_\\tt$\n",
    "        - feeds $y_{(1:\\tt)}$ back as input for iteration $\\tt+1$\n",
    "        - question $\\x$ is a prefix of $\\y$\n",
    "        $$ \\y' = \\text{concat}(\\x, \\y)$$\n",
    "    - no cross-attention to the Encoder (because no Encoder)\n",
    "        - just self-attention to incrementally generated $\\y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In a retrieval Transformer (non-parametric knowledge)\n",
    "- Encoder-Decoder style Transformer\n",
    "- at inference time\n",
    "    - question $\\x$ is sent to Retriever\n",
    "        - returns $K$ relevant items\n",
    "        - the $K$ relevant items, and $\\x$,  are input to the Encoder\n",
    "- Once the Encoder finishes, the Decoder operates auto-regressively\n",
    "    - just as above\n",
    "    - but with Cross-Attention to Encoder output (retrieved knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Notation**\n",
    "\n",
    "| Variable | Definition |\n",
    "|:---|:---|\n",
    "$n$          | maximum text length\n",
    "|            | example: $n=2048$\n",
    "$m$          | chunk of text length\n",
    "|            | text of length $n$ broken up into $l = \\frac{n}{m}$ chunks of length $m$\n",
    "|            | example: $m=64$\n",
    "$\\mathcal{D}$ | Knowledge store\n",
    "|             | Collection of items\n",
    "|            | implemented as key/value pairs\n",
    "|            | Item $i$ has key $N_i$ and value $F_i$\n",
    "|            | $N_i$ is a chunk of text; $F_i$ is the following chunk of text\n",
    "| $T$        | number of items in $\\mathcal{D}$\n",
    "|            | $T = 2 * 10^{12}$\n",
    "$r$          | length of returned item\n",
    "|            | $r = 2 * m = 128$\n",
    "$k$          | number of similar items returned\n",
    "|            | size of set $\\z$\n",
    "|            | example: $k=40$\n",
    "$\\text{Ret}(C)$    | set $\\z$ of returned itemes\n",
    "|            | dimension $(k \\times r)$\n",
    "| $\\y$       | The partially built output sequence\n",
    "|            | - starts with question $\\x$\n",
    "|            | - is extended by the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model details: chunked data\n",
    "\n",
    "The model works by breaking long text into *chunks* of length $m$.\n",
    "\n",
    "Thus, the items in the Knowledge store are chunks (\"passages\" as we previously called them) not full documents.\n",
    "\n",
    "Similarly, the question $\\x$ and partially generated answer $\\y$ are also broken up into chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Retriever**\n",
    "\n",
    "The Knowledge Store is implemented as Key/Value Pairs\n",
    "- the element $i$\n",
    "    - key $N^i$ is a chunk\n",
    "    - value $F^i$ is the chunk that immediately follows $N_i$\n",
    "\n",
    "Each key (and query against the Key/Value pairs)\n",
    "- is encoded by a BERT transformer (with averaging over \"time\" == tokens).\n",
    "\n",
    "Lookup with with query $q$ returns $k$ items\n",
    "- each item is $[N^i, F^i]$ where distance between query $C$ and key $N$ is among the $k$ smallest\n",
    "    - $d(C, N) = || \\text{BERT}(C) - \\text{BERT}(N) ||^2_2$    \n",
    "    - $\n",
    "\\text{Ret}(C) = \\left( [N^1, F^1], \\ldots, [N^k, F^k]  \\right)\n",
    "$\n",
    "    - length of item is $r = 2 * m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Generator**\n",
    "\n",
    "Just like a Standard LM with a \"predict the next\" token objective\n",
    "- the generator *autoregressively* generates next output token $\\y_\\ip$ \n",
    "    - conditioned on all previous output tokens $\\y_{(1:i-1)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In addition\n",
    "- $\\y_{(1:\\tt-1)}$ is broken into $l = \\frac{\\tt-1}{m}$ chunks of length $m$\n",
    "$$\n",
    "\\y_{(1:\\tt-1)} = C_1, \\ldots, C_{l}\n",
    "$$\n",
    "- a set of $k$ items is retrieved for each chunk $C_j$\n",
    "$$\n",
    "\\text{Ret}(C_i)\n",
    "$$\n",
    "\n",
    "So the generator is *also* conditioned on\n",
    "$$\n",
    "\\text{Ret}(C_i), \\ldots, \\text{Ret}(C_l)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Generator maximizes the likelihood of the next output $\\y_\\tt$ conditioned on \n",
    "- previously generated partial $\\y_{(1:\\tt-1)}$\n",
    "- and items retrieved from the chunks accessible to $\\y_{(1:\\tt-1)}$ \n",
    "    - let $u$ denote the index of the last chunk accessible to $\\y_{(1:\\tt-1)}$ \n",
    "$$\n",
    "\\pr{ \\y_i \\; | \\;  \\y_{(1:i-1)}, \\{ \\text{Ret}(C_{u'})\\, |\\,  u' \\lt u \\}                     }\n",
    "$$\n",
    "\n",
    "Fine point about \"chunks accessible to $\\y_{(1:\\tt-1)}$\"\n",
    "- if $\\y_{(t-1)}$ is not the *last* item in the chunk\n",
    "- then the chunk containing  $\\y_{(t-1)}$ includes $\\y_{\\tt'}$ for $\\tt' > \\tt-1$\n",
    "- can't use a chunk if it includes $\\y_{\\tt'}$ for $\\tt' > \\tt-1$ because it violates causality\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The paper expresses this\n",
    "- as log-likelihoods $\\mathbb{L}$\n",
    "    - log so we can use sum rather than product\n",
    "- indexes elements of $\\y$ using the chunk number $u$ and offset $j$ within chunk\n",
    "    $$i = (u-1)*m + j$$\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\mathbb{L} \\left( \\y_i \\; | \\;  \\y_{(1:i-1)}, \\{ \\text{Ret}(C_{u'})\\, |\\,  u' \\lt u \\}    \\right) \n",
    "& = &\n",
    "\\sum_{u=1}^l {\n",
    "    \\sum_{j=1}^m { \\mathbb{L} \\left( \\y_{\\left( (u-1)*m+j \\right) } \\; | \\; \\y_{\\left(1:(u-1)*m+j -1 \\right) } , \\{ \\text{Ret}(C_{u'})\\, |\\,  u' \\lt u \\}\n",
    "                               \\right)\n",
    "    }\n",
    "}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RETRO-fitting existing models\n",
    "\n",
    "The authors have had success adapting non-retrieval models to use RETRO retrieval\n",
    "- freeze weights of non-retrieval model\n",
    "- train only\n",
    "    - Chunked Cross Attention\n",
    "    - Neighbor Encoder\n",
    "    - this training is on dataset that is only 3% as big as the full training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [WebGPT (OpenAI)](https://openai.com/blog/webgpt)\n",
    "\n",
    "[paper](https://arxiv.org/abs/2112.09332)\n",
    "\n",
    "WebGPT uses the Web as its source of World Knowledge.\n",
    "\n",
    "In order to better be able to evaluate the truthfulness of answers\n",
    "- specific passages (called *references*) are extracted from Web pages\n",
    "- answer uses the references as support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model summary: high-level (approximate)\n",
    "\n",
    "Retriever\n",
    "- **dynamic** non-parametric knowledge\n",
    "    - constantly changing Web\n",
    "\n",
    "Generator\n",
    "- GPT style LLM (i.e., Decoder only)\n",
    "- Takes question $\\x$ and supporting references $\\z$ returned by Retriever\n",
    "    - answer extends the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model details\n",
    "\n",
    "The novelty is how the Retriever is able to \"browse the Web\".\n",
    "\n",
    "Basically\n",
    "- a human demonstrates \n",
    "    - how to use a browser to search the Web\n",
    "    - in order to gather references $\\z$ that are relevant for answering question $\\x$\n",
    "- the model learns how to imitate the human's behavior.\n",
    "\n",
    "This technique is called *Behavioral Cloning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Retriever is trained with examples that are human-created *demonstrations* of behavior.\n",
    "\n",
    "The human's behavior (sequence of actions) is recorded as the human interacts with a Browser.\n",
    "- Issue a query to the browser\n",
    "- Extract relevant *references* (passages) from the query results\n",
    "- End the Web search and move to generating an answer using the collected references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a example of the behavioral actions from  [WebGPT](https://openai.com/blog/webgpt)\n",
    "\n",
    "| Command | Effect |\n",
    "|:---|:---|\n",
    "Search `<query>`             | Send `<query>` to Bing API\n",
    "Click on link `<link ID>`    | Follow the link\n",
    "Find in page `<text>`        | Find next occurrence of `<text>` and scroll to it\n",
    "Quote: `<text>`                | If `Find` successful: add it as a reference\n",
    "Scroll down | Navigate through page\n",
    "Scroll up   |\n",
    "Top         |\n",
    "Back        |\n",
    "End: Answer | End browsing and move to answering phase\n",
    "End: `<Nonsense, Controversial>` | End browsing and skip answering phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The behavior is encoded as text and, along with the question, forms an example.\n",
    "\n",
    "Here is the interface (left) used by a human to record behavior and the encoded behavior (right)\n",
    "\n",
    "<img src=\"images/WebGPT_text_browsing_xface.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Encoded fields:\n",
    "- Question\n",
    "    `how can I train the crows in my neighborhood to bring me gifts ?`\n",
    "- Past actions\n",
    "    - Web query: `Search \"how to train crows to bring you gifts\"`\n",
    "- Text\n",
    "    - results returned by Web query\n",
    "- Next action\n",
    "    - prompt for LLM to complete, by predicting next action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Retriever is trained with examples that are a prefix of a full demonstration\n",
    "- learns how to extend the behavior with a new action\n",
    "    - \"predict the next\" action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since a machine can't \"see\" the screen\n",
    "- the browser context and state is recorded as a written summary of the environment\n",
    "\n",
    "Similarly, it can't \"remember\" the past actions\n",
    "- these too are recorded as text\n",
    "\n",
    "Using this textual encoding of the history of actions, the LLM tries to extend the behavior via a new action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generator\n",
    "\n",
    "The Generator is trained to create answers \n",
    "- that cite the references\n",
    "\n",
    "Because the references are text (just like the questions)\n",
    "- there is no preprocessing of the retrieved items necessary\n",
    "    - compare to RETRO which needs to process retrieved passages through the Encoder\n",
    "        - in order to facilitate Decoder-Encoder cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Questions\n",
    "\n",
    "There are some questions as to the exact details\n",
    "- what is the syntax of the prompt ?\n",
    "    - identifying the parts: passage, question\n",
    "- how many passages are used ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Most significant: is the task **zero**-shot or **few**-shot\n",
    "- zero-shot: prompt consists only of the particular question $q$ (and passages)\n",
    " ```\n",
    "**Passage**:  <...>\n",
    "**Question**: <...>\n",
    "**Answer**:  \n",
    "```\n",
    "with the LLM expected to extend the text beyond the final `**Answer**:` \n",
    "- few-shot:\n",
    "    - the particular question and passage is preceded by $k >0$ run-time examples\n",
    "    - suggesting the task is to complete the answer based on references\n",
    "    - e.g., here is an illustration of one of the $k$ examples\n",
    "```\n",
    "**Passage**:  <...>\n",
    "**Question**: <...>\n",
    "**Answer**:   <..>\n",
    "```\n",
    "        - suggesting the goal of completing the Answer based on references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although I can't find the precise details\n",
    "- the next section on Internet Augmented Language models is similar to WebGPT\n",
    "- with more detail\n",
    "\n",
    "so is suggestive of the details for WebGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other model characteristics (out of scope for this module)\n",
    "\n",
    "\n",
    "There is a lot more to WebGPT than just the use of a dynamic, non-parametric knowledge store.\n",
    "\n",
    "These are beyond the scope of the present topic, but we briefly describe some interesting characteristics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reward Model\n",
    "\n",
    "There is a desire to produce answers that are helpful, truthful, non-harmful and high quality.\n",
    "\n",
    "None of these are explicit objectives of a LLM.\n",
    "\n",
    "The authors fine-tune the LLM towards this end.\n",
    "\n",
    "The idea is to train a *reward model* to predict which of two answers is \"better\".\n",
    "\n",
    "Given the reward model, the authors use *Reinforcement Learning with Human Feedback* to fine-tune\n",
    "the LLM in the direction of producing better answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Human Feedback comes from\n",
    "- having the initial LLM generate multiple answers to a question\n",
    "- having a human rank the answers\n",
    "\n",
    "A question and two ranked answers become an example used \n",
    "- to train a Classifier\n",
    "- to predict which answer is better.\n",
    "\n",
    "This is called the *Reward Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the interface with which a Human evaluates multiple answers\n",
    "\n",
    "<img src=\"images/WebGPT_Comparison_xface.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reinforcement Learning to fine-tune the LLM\n",
    "\n",
    "In the improvement phase, the initial LLM\n",
    "- generates an answer\n",
    "- the answer's quality is predicted by the Reward Model\n",
    "\n",
    "The LLM parameters are adjusted by *Reinforcement Learning*\n",
    "- - parameters are adjusted so as to increase Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rejection sampling\n",
    "\n",
    "The model is asked to produce several answers\n",
    "- each is evaluated by the Reward model\n",
    "- the answer with highest reward is selected\n",
    "\n",
    "Rejection sampling can be used either on\n",
    "- the initial LLM (before Reinforcement Learning)\n",
    "- after Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Internet Augmented Language Models (DeepMind)](https://arxiv.org/pdf/2203.05115.pdf)\n",
    "\n",
    "This model is similar to WebGPT\n",
    "- but developed by Google\n",
    "- uses Google search rather than Bing\n",
    "\n",
    "There is more detail in this paper than the one for WebGPT\n",
    "- perhaps the answer to the open questions we had for WebGPT have similar answers to what we find here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Retriever\n",
    "\n",
    "- Google as the search Engine\n",
    "- Question $q$ passed *verbatim* (unchanged) to the Search Engine\n",
    "- Search Engine returns the URL of the top 20 results\n",
    "- The results are converted to text and broken into paragraphs of 6 sentences\n",
    "- The similarity of the retrieved paragraphs is compared to $q$\n",
    "- The top 50 paragraphs form the set $\\z$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We contrast the query used to that of WebGPT\n",
    "- Google: unchanged from question $q$\n",
    "- WebGPT: \"learning to query\" approach\n",
    "    - Model trained to search the Web\n",
    "\n",
    "A justification given for using the original question $q$ as query\n",
    "\n",
    "Apparently: \n",
    "- most search engines perform some type of query transformation to improve user experience\n",
    "- hidden from user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also compare how relevant passages are extracted\n",
    "- Google: chunks, ranked by similarity to question\n",
    "- WebGPT: learns to extract passages (via demonstrations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generator\n",
    "\n",
    "### k-shot Prompting ($k = 15$)\n",
    "\n",
    "The LLM has not been trained for the particular task of Question Answering.\n",
    "\n",
    "Thus, it needs to be *conditioned* on this task by being shown $k$ examples of question/evidence/answer.\n",
    "\n",
    "The format of each prototype example is \n",
    "\n",
    "```\n",
    "**Evidence**: <...>\n",
    "**Question**: <...>\n",
    "**Answers**:  <...>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The particular questions $q$ is appended to the $k$ prototype examples\n",
    "- but without anything following `**Answer**:`\n",
    "- the LLM will provide the answer by extending the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Number of passages/Number of answers\n",
    "\n",
    "Only a *single* passage is used as evidence at a time\n",
    "- the model creates *multiple* answers from question $q$ and *each* passage\n",
    "- $a_{i,j}$ denotes answer $j$ based on the evidence $z_i$: element $i$ of $\\z$\n",
    "\n",
    "\n",
    "There are $50 * 4$ answers to each question $q$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is a *scoring function* that ranks each of the answers.\n",
    "\n",
    "The answer with the highest score is chosen as the final answer\n",
    "$$\n",
    "\\y = \\max{a_{i,j}} f(q, z_i, j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors experiment with different scoring functions.\n",
    "\n",
    "The simplest: the answer with highest model probability\n",
    "- recall: \n",
    "    - LLM generates output one token at a time\n",
    "    - Each token is drawn from a probability distribution\n",
    "    - Can thus derive probability of $\\y$ by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "800px",
    "left": "243px",
    "top": "275.125px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
